{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math, random\n",
    "from einops import rearrange\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "mask_idx = tokenizer.mask_token_id\n",
    "sep_idx = tokenizer.sep_token_id\n",
    "vocab_size = tokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LAMBDA = 0\n",
    "EPOCH = 15\n",
    "max_len = 100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "warmup_steps = 2000\n",
    "LR_peak = 2.5e-4\n",
    "total_steps = EPOCH * math.ceil(97000/BATCH_SIZE)\n",
    "\n",
    "def lr_lambda(step):\n",
    "    return min(step / warmup_steps, (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps)))))\n",
    "\n",
    "save_model_path = '/results/GPT_small.pt'\n",
    "save_history_path = '/results/GPT_small_history.pt'\n",
    "n_layers = 12\n",
    "d_model = 768\n",
    "d_ff = d_model * 4\n",
    "n_heads = 12\n",
    "drop_p = 0.25 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('대화체.xlsx')\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.loc[idx, '원문']\n",
    "\n",
    "custom_DS = CustomDataset(data)\n",
    "\n",
    "train_DS, val_DS, test_DS, _ = torch.utils.data.random_split(custom_DS, [97000, 2000, 1000, len(custom_DS)-97000-2000-1000])\n",
    "\n",
    "train_DL = torch.utils.data.DataLoader(train_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_DL = torch.utils.data.DataLoader(val_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_DL = torch.utils.data.DataLoader(test_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(d_model, d_model)\n",
    "        self.fc_k = nn.Linear(d_model, d_model)\n",
    "        self.fc_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.scale = torch.sqrt(torch.tensor(d_model / n_heads))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        Q = self.fc_q(x)\n",
    "        K = self.fc_k(x)\n",
    "        V = self.fc_v(x)\n",
    "\n",
    "        Q = rearrange(Q, '개 단 (헤 차) -> 개 헤 단 차', 헤 = self.n_heads)\n",
    "        K = rearrange(K, '개 단 (헤 차) -> 개 헤 단 차', 헤 = self.n_heads)\n",
    "        V = rearrange(V, '개 단 (헤 차) -> 개 헤 단 차', 헤 = self.n_heads)\n",
    "\n",
    "        attention_score = Q @ K.transpose(-2,-1) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_score[mask] = -1e10\n",
    "        attention_weights = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        attention = attention_weights @ V\n",
    "\n",
    "        x = rearrange(attention, '개 헤 단 차 -> 개 단 (헤 차)')\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, attention_weights\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                                    nn.GELU(),\n",
    "                                    nn.Dropout(drop_p),\n",
    "                                    nn.Linear(d_ff, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_atten_LN = nn.LayerNorm(d_model)\n",
    "        self.self_atten = MHA(d_model, n_heads, drop_p)\n",
    "\n",
    "        self.FF_LN = nn.LayerNorm(d_model)\n",
    "        self.FF = FeedForward(d_model, d_ff, drop_p)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, x, dec_mask):\n",
    "\n",
    "        residual = self.self_atten_LN(x)\n",
    "        residual, atten_dec = self.self_atten(residual, dec_mask)\n",
    "        residual = self.dropout(residual)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = self.FF_LN(x)\n",
    "        residual = self.FF(residual)\n",
    "        residual = self.dropout(residual)\n",
    "        x = x + residual\n",
    "\n",
    "        return x, atten_dec\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dout_p, seq_len=3660):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dout_p)\n",
    "\n",
    "        pos_enc_mat = np.zeros((seq_len, d_model))\n",
    "        odds = np.arange(0, d_model, 2)\n",
    "        evens = np.arange(1, d_model, 2)\n",
    "\n",
    "        for pos in range(seq_len):\n",
    "            pos_enc_mat[pos, odds] = np.sin(pos / (10000 ** (odds / d_model)))\n",
    "            pos_enc_mat[pos, evens] = np.cos(pos / (10000 ** (evens / d_model)))\n",
    "\n",
    "        self.pos_enc_mat = torch.from_numpy(pos_enc_mat).unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, d_model = x.shape\n",
    "        \n",
    "        x = x + self.pos_enc_mat[:, :S, :].type_as(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, n_layers, d_model, d_ff, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.pos_encoding = PositionalEncoder(d_model, drop_p, max_len)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, d_ff, n_heads, drop_p) for _ in range(n_layers)])\n",
    "\n",
    "        self.LN_out = nn.LayerNorm(d_model)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, dec_mask, atten_map_save = False):\n",
    "\n",
    "        # pos = torch.arange(x.shape[1]).expand_as(x).to(device)\n",
    "\n",
    "        # x = self.input_embedding(x) + self.pos_embedding(pos)\n",
    "        x = self.input_embedding(x) + self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        atten_decs = torch.tensor([]).to(device)\n",
    "        for layer in self.layers:\n",
    "            x, atten_dec = layer(x, dec_mask)\n",
    "            if atten_map_save is True:\n",
    "                atten_decs = torch.cat([atten_decs , atten_dec[0].unsqueeze(0)], dim=0)\n",
    "\n",
    "        x = self.LN_out(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x, atten_decs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, n_layers, d_model, d_ff, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = Decoder(vocab_size, max_len, n_layers, d_model, d_ff, n_heads, drop_p)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.02)\n",
    "                m.weight.data *= 1/torch.sqrt(torch.tensor(n_layers*2))\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.02)\n",
    "\n",
    "        nn.init.normal_(self.decoder.fc_out.weight, mean=0, std=0.02)\n",
    "\n",
    "    def make_dec_mask(self, x):\n",
    "\n",
    "        pad_mask = (x == pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        pad_mask = pad_mask.expand(x.shape[0], self.n_heads, x.shape[1], x.shape[1])\n",
    "        future_mask = torch.tril(torch.ones(x.shape[0], self.n_heads, x.shape[1], x.shape[1]))==0\n",
    "        future_mask = future_mask.to(device)\n",
    "        dec_mask = pad_mask | future_mask \n",
    "\n",
    "        return dec_mask\n",
    "\n",
    "    def forward(self, x, atten_map_save = False):\n",
    "\n",
    "        dec_mask = self.make_dec_mask(x)\n",
    "        out, atten_decs = self.decoder(x, dec_mask, atten_map_save = atten_map_save)\n",
    "\n",
    "        return out, atten_decs\n",
    "    \n",
    "model = GPT(vocab_size, max_len, n_layers, d_model, d_ff, n_heads, drop_p).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
