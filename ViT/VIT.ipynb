{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LAMBDA = 3e-1 \n",
    "EPOCH = 50\n",
    "\n",
    "scheduler_name = 'Cos'\n",
    "warmup_steps = 1000\n",
    "LR_scale = 0.5\n",
    "warmup_steps = 12000\n",
    "LR_peak = 1e-3\n",
    "LR_init = 1e-4\n",
    "LR = 1e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "save_model_path = '/results/ViT_CIFAR10_2.pt'\n",
    "save_history_path = '/results/ViT_CIFAR10_2_history.pt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # transforms.RandomResizedCrop(scale=(0.9,1), ratio=(0.3,1.7), size=(32,32))\n",
    "\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05)\n",
    "\n",
    "    transforms.RandomHorizontalFlip(p=0.4), \n",
    "    # transforms.RandomVerticalFlip(p=0.5)\n",
    "    # transforms.RandomAffine(degrees=(0,90),translate=(0.1,0.2),scale=(0.9,1.1))\n",
    "    # transforms.RandomPerspective(distortion_scale=0.1, p=0.1)\n",
    "\n",
    "    transforms.ToTensor(), \n",
    "\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.03,0.08), ratio=(0.3,3.3)), \n",
    "    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.ToTensor()\n",
    "\n",
    "train_DS = datasets.CIFAR10(root = './data', train=True, download=True, transform=transform_train)\n",
    "train_DS, val_DS = torch.utils.data.random_split(train_DS, [45000, 5000])\n",
    "test_DS = datasets.CIFAR10(root = './data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_DL = torch.utils.data.DataLoader(train_DS, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_DL = torch.utils.data.DataLoader(val_DS, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_DL = torch.utils.data.DataLoader(test_DS, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.tensor(hidden_dim / n_heads))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc_q.weight)\n",
    "        nn.init.xavier_uniform_(self.fc_k.weight)\n",
    "        nn.init.xavier_uniform_(self.fc_v.weight)\n",
    "        nn.init.xavier_uniform_(self.fc_o.weight)\n",
    "\n",
    "        if self.fc_q.bias is not None:\n",
    "            nn.init.constant_(self.fc_q.bias, 0)\n",
    "        if self.fc_k.bias is not None:\n",
    "            nn.init.constant_(self.fc_k.bias, 0)\n",
    "        if self.fc_v.bias is not None:\n",
    "            nn.init.constant_(self.fc_v.bias, 0)\n",
    "        if self.fc_o.bias is not None:\n",
    "            nn.init.constant_(self.fc_o.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        Q = self.fc_q(x)\n",
    "        K = self.fc_k(x)\n",
    "        V = self.fc_v(x)\n",
    "\n",
    "        Q = rearrange(Q, '개 단 (헤 차) -> 개 헤 단 차', 헤 = self.n_heads)\n",
    "        K = rearrange(K, '개 단 (헤 차) -> 개 헤 단 차', 헤 = self.n_heads)\n",
    "        V = rearrange(V, '개 단 (헤 차) -> 개 헤 단 차', 헤 = self.n_heads)\n",
    "\n",
    "        attention_score = Q @ K.transpose(-2,-1)/self.scale\n",
    "\n",
    "        attention_weights = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        attention = attention_weights @ V\n",
    "\n",
    "        x = rearrange(attention, '개 헤 단 차 -> 개 단 (헤 차)')\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, attention_weights\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, d_ff, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(hidden_dim, d_ff),\n",
    "                                    nn.GELU(),\n",
    "                                    nn.Dropout(drop_p),\n",
    "                                    nn.Linear(d_ff, hidden_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, d_ff, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_atten_LN = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "        self.self_atten = MHA(hidden_dim, n_heads)\n",
    "\n",
    "        self.FF_LN = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "        self.FF = FeedForward(hidden_dim, d_ff, drop_p)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = self.self_atten_LN(x)\n",
    "        residual, atten_enc = self.self_atten(residual)\n",
    "        residual = self.dropout(residual)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = self.FF_LN(x)\n",
    "        residual = self.FF(residual)\n",
    "        residual = self.dropout(residual)\n",
    "        x = x + residual\n",
    "\n",
    "        return x, atten_enc\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_length, n_layers, hidden_dim, d_ff, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(0.02*torch.randn(seq_length, hidden_dim))\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, d_ff, n_heads, drop_p) for _ in range(n_layers)])\n",
    "        self.ln = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, src, atten_map_save = False):\n",
    "\n",
    "        x = src + self.pos_embedding.expand_as(src)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        atten_encs = torch.tensor([]).to(device)\n",
    "        for layer in self.layers:\n",
    "            x, atten_enc = layer(x)\n",
    "            if atten_map_save is True:\n",
    "                atten_encs = torch.cat([atten_encs , atten_enc[0].unsqueeze(0)], dim=0)\n",
    "\n",
    "        x = x[:,0,:]\n",
    "        x = self.ln(x)\n",
    "\n",
    "        return x, atten_encs\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, n_layers, hidden_dim, d_ff, n_heads, representation_size = None, drop_p = 0., num_classes = 1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        seq_length = (image_size // patch_size) ** 2 + 1\n",
    "\n",
    "        self.class_token = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.input_embedding = nn.Conv2d(3, hidden_dim, patch_size, stride=patch_size)\n",
    "        self.encoder = Encoder(seq_length, n_layers, hidden_dim, d_ff, n_heads, drop_p)\n",
    "\n",
    "        heads_layers = []\n",
    "        if representation_size is None:\n",
    "            self.head = nn.Linear(hidden_dim, num_classes)\n",
    "        else:\n",
    "            self.head = nn.Sequential(nn.Linear(hidden_dim, representation_size),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(representation_size, num_classes))\n",
    "\n",
    "        fan_in = self.input_embedding.in_channels * self.input_embedding.kernel_size[0] * self.input_embedding.kernel_size[1]\n",
    "        nn.init.trunc_normal_(self.input_embedding.weight, std=math.sqrt(1 / fan_in))\n",
    "        if self.input_embedding.bias is not None:\n",
    "            nn.init.zeros_(self.input_embedding.bias)\n",
    "\n",
    "        if representation_size is None:\n",
    "            nn.init.zeros_(self.head.weight)\n",
    "            nn.init.zeros_(self.head.bias)\n",
    "        else:\n",
    "            fan_in = self.head[0].in_features\n",
    "            nn.init.trunc_normal_(self.head[0].weight, std=math.sqrt(1 / fan_in))\n",
    "            nn.init.zeros_(self.head[0].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.input_embedding(x)\n",
    "        x = rearrange(x, '개 차 단h 단w -> 개 (단h 단w) 차')\n",
    "\n",
    "        batch_class_token = self.class_token.expand(x.shape[0], 1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "        enc_out, atten_encs = self.encoder(x)\n",
    "\n",
    "        x = self.head(enc_out)\n",
    "\n",
    "        return x, atten_encs\n",
    "\n",
    "# def vit_b_16(**kwargs):\n",
    "#     return VisionTransformer(image_size = 224, patch_size = 16, n_layers = 12, hidden_dim = 768, d_ff = 3072, n_heads = 12, representation_size = 768, **kwargs)\n",
    "\n",
    "# def vit_b_32(**kwargs):\n",
    "#     return VisionTransformer(image_size = 224, patch_size = 32, n_layers = 12, hidden_dim = 768, d_ff = 3072, n_heads = 12, representation_size = 768, **kwargs)\n",
    "\n",
    "# def vit_l_16(**kwargs):\n",
    "#     return VisionTransformer(image_size = 224, patch_size = 16, n_layers = 24, hidden_dim = 1024, d_ff = 4096, n_heads = 16, representation_size = 1024, **kwargs)\n",
    "\n",
    "# def vit_l_32(**kwargs):\n",
    "#     return VisionTransformer(image_size = 224, patch_size = 32, n_layers = 24, hidden_dim = 1024, d_ff = 4096, n_heads = 16, representation_size = 1024, **kwargs)\n",
    "\n",
    "# def vit_h_14(**kwargs):\n",
    "#     return VisionTransformer(image_size = 224, patch_size = 14, n_layers = 32, hidden_dim = 1280, d_ff = 5120, n_heads = 16, representation_size = 1280, **kwargs)\n",
    "\n",
    "def vit_cifar10(**kwargs):\n",
    "    return VisionTransformer(image_size = 32, patch_size = 2, n_layers = 3, hidden_dim = 256, d_ff = 1024, n_heads = 8, representation_size = 256, **kwargs)\n",
    "\n",
    "model = vit_cifar10().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jung-yechan/.pyenv/versions/3.9.20/envs/dl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VisionTransformer                             [2, 1000]                 256\n",
       "├─Conv2d: 1-1                                 [2, 256, 16, 16]          3,328\n",
       "├─Encoder: 1-2                                [2, 256]                  65,792\n",
       "│    └─Dropout: 2-1                           [2, 257, 256]             --\n",
       "│    └─ModuleList: 2-2                        --                        --\n",
       "│    │    └─EncoderLayer: 3-1                 [2, 257, 256]             789,760\n",
       "│    │    └─EncoderLayer: 3-2                 [2, 257, 256]             789,760\n",
       "│    │    └─EncoderLayer: 3-3                 [2, 257, 256]             789,760\n",
       "│    └─LayerNorm: 2-3                         [2, 256]                  512\n",
       "├─Sequential: 1-3                             [2, 1000]                 --\n",
       "│    └─Linear: 2-4                            [2, 256]                  65,792\n",
       "│    └─Tanh: 2-5                              [2, 256]                  --\n",
       "│    └─Linear: 2-6                            [2, 1000]                 257,000\n",
       "===============================================================================================\n",
       "Total params: 2,761,960\n",
       "Trainable params: 2,761,960\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 7.09\n",
       "===============================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 35.81\n",
       "Params size (MB): 10.78\n",
       "Estimated Total Size (MB): 46.62\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(2,3,32,32), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Test, loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model, train_DL, val_DL, criterion, optimizer, scheduler = None):\n",
    "    loss_history = {\"train\": [], \"val\": []}\n",
    "    acc_history = {\"train\": [], \"val\": []}\n",
    "    best_loss = 9999\n",
    "    for ep in range(EPOCH):\n",
    "        model.train() \n",
    "        train_loss, train_acc, _ = loss_epoch(model, train_DL, criterion, optimizer = optimizer, scheduler = scheduler)\n",
    "        loss_history[\"train\"] += [train_loss]\n",
    "        acc_history[\"train\"] += [train_acc]\n",
    "\n",
    "        model.eval() \n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc, _ = loss_epoch(model, val_DL, criterion)\n",
    "            loss_history[\"val\"] += [val_loss]\n",
    "            acc_history[\"val\"] += [val_acc]\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save({\"model\": model,\n",
    "                            \"ep\": ep,\n",
    "                            \"optimizer\": optimizer,\n",
    "                            \"scheduler\": scheduler}, save_model_path)\n",
    "        print(f\"Epoch: {ep+1}, current_LR = {optimizer.param_groups[0]['lr']:.8f}\")\n",
    "        print(f\"train loss: {train_loss:.5f}, \"\n",
    "              f\"val loss: {val_loss:.5f} \\n\"\n",
    "              f\"train acc: {train_acc:.1f} %, \"\n",
    "              f\"val acc: {val_acc:.1f} %\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    torch.save({\"loss_history\": loss_history,\n",
    "                \"acc_history\": acc_history,\n",
    "                \"EPOCH\": EPOCH,\n",
    "                \"BATCH_SIZE\": BATCH_SIZE}, save_history_path)\n",
    "\n",
    "def Test(model, test_DL, criterion):\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        test_loss, test_acc, rcorrect = loss_epoch(model, test_DL, criterion)\n",
    "    print(f\"Test loss: {test_loss:.3f}\")\n",
    "    print(f\"Test accuracy: {rcorrect}/{len(test_DL.dataset)} ({round(test_acc,1)} %)\")\n",
    "    return round(test_acc,1)\n",
    "\n",
    "def loss_epoch(model, DL, criterion, optimizer = None, scheduler = None):\n",
    "    N = len(DL.dataset) \n",
    "    rloss=0; rcorrect = 0\n",
    "    for x_batch, y_batch in tqdm(DL, leave=False):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_hat = model(x_batch)[0]\n",
    "        loss = criterion(y_hat, y_batch)\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        loss_b = loss.item() * x_batch.shape[0]\n",
    "        rloss += loss_b\n",
    "        pred = y_hat.argmax(dim=1)\n",
    "        corrects_b = torch.sum(pred == y_batch).item()\n",
    "        rcorrect += corrects_b\n",
    "    loss_e = rloss/N\n",
    "    accuracy_e = rcorrect/N * 100\n",
    "\n",
    "    return loss_e, accuracy_e, rcorrect\n",
    "\n",
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer, hidden_dim, warmup_steps, LR_scale = 1):\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.LR_scale = LR_scale\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lrate = self.LR_scale * (self.hidden_dim ** -0.5) * min(self.current_step ** -0.5, self.current_step * self.warmup_steps ** -1.5)\n",
    "        self.optimizer.param_groups[0]['lr'] = lrate\n",
    "\n",
    "class LinearWarmupLinearDecayScheduler:\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, max_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.max_lr = max_lr\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            lrate = self.max_lr * (self.current_step / self.warmup_steps)\n",
    "        else:\n",
    "            decay_steps = self.total_steps - self.warmup_steps\n",
    "            lrate = self.max_lr * max(0, float(decay_steps - (self.current_step - self.warmup_steps)) / decay_steps)\n",
    "        self.optimizer.param_groups[0]['lr'] = lrate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(nn.Linear(1, 1).parameters(), lr=LR_init)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max = int(len(train_DS)*EPOCH/BATCH_SIZE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad] \n",
    "if scheduler_name == 'Noam':\n",
    "    optimizer = optim.AdamW(params, lr=0, weight_decay=LAMBDA)\n",
    "    scheduler = NoamScheduler(optimizer, hidden_dim=model.hidden_dim, warmup_steps=warmup_steps, LR_scale=LR_scale)\n",
    "elif scheduler_name == 'Linear':\n",
    "    optimizer = optim.AdamW(params, lr=0, weight_decay=LAMBDA)\n",
    "    scheduler = LinearWarmupLinearDecayScheduler(optimizer, warmup_steps=warmup_steps, total_steps = int(len(train_DS)*EPOCH/BATCH_SIZE), max_lr=LR_peak)\n",
    "elif scheduler_name == 'Cos':\n",
    "    optimizer = optim.AdamW(params, lr=LR_init, weight_decay=LAMBDA)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max = int(len(train_DS)*EPOCH/BATCH_SIZE))\n",
    "elif scheduler_name == 'Constant':\n",
    "    optimizer = optim.AdamW(params, lr=LR, weight_decay=LAMBDA)\n",
    "    scheduler = None\n",
    "Train(model, train_DL, val_DL, criterion, optimizer, scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
