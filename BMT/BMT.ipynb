{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class FeatureEmbedder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_feat, d_model):\n",
    "        super(FeatureEmbedder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedder = nn.Linear(d_feat, d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, S, d_model_m) <- (B, S, D_original_feat_dim)\n",
    "        x = self.embedder(x)\n",
    "        x = x * np.sqrt(self.d_model)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # (B, S, d_model_m)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dout_p, seq_len=3660):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dout_p)\n",
    "\n",
    "        pos_enc_mat = np.zeros((seq_len, d_model))\n",
    "        odds = np.arange(0, d_model, 2)\n",
    "        evens = np.arange(1, d_model, 2)\n",
    "\n",
    "        for pos in range(seq_len):\n",
    "            pos_enc_mat[pos, odds] = np.sin(pos / (10000 ** (odds / d_model)))\n",
    "            pos_enc_mat[pos, evens] = np.cos(pos / (10000 ** (evens / d_model)))\n",
    "\n",
    "        self.pos_enc_mat = torch.from_numpy(pos_enc_mat).unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, d_model = x.shape\n",
    "        x = x + self.pos_enc_mat[:, :S, :].type_as(x)\n",
    "        x = self.dropout(x)\n",
    "        # same as input\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerStack(nn.Module):\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(LayerStack, self).__init__()\n",
    "        self.layers = clone(layer, N)\n",
    "\n",
    "    def forward(self, x, masks):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, masks)\n",
    "        return x\n",
    "\n",
    "def clone(module, N):\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(N)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model_Q, d_model_K, d_model_V, dout_p=0.0, d_model=256, n_heads=8):\n",
    "        super(MHA, self).__init__()\n",
    "        self.d_model_Q = d_model_Q\n",
    "        self.d_model_K = d_model_K\n",
    "        self.d_model_V = d_model_V\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.dout_p = dout_p\n",
    "\n",
    "        if self.d_model is None:\n",
    "            print(f'd_model: is None')\n",
    "            self.d_model = self.d_model_Q\n",
    "\n",
    "        self.d_k = self.d_model // self.n_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(self.d_model_Q, self.d_model)\n",
    "        self.fc_k = nn.Linear(self.d_model_K, self.d_model)\n",
    "        self.fc_v = nn.Linear(self.d_model_V, self.d_model)\n",
    "        self.fc_o = nn.Linear(self.d_model, self.d_model_Q)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dout_p)\n",
    "        self.scale = torch.sqrt(torch.tensor(self.d_model / self.n_heads))\n",
    "\n",
    "        assert self.d_model % H == 0\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        ''' \n",
    "            Q, K, V: (B, Sq, Dq), (B, Sk, Dk), (B, Sv, Dv)\n",
    "            mask: (B, 1, Sk)\n",
    "            Sk = Sv, \n",
    "            Dk != self.d_k\n",
    "            Also: m1 is the target modality (queries); m2 is the source modality (keys, values)\n",
    "        '''\n",
    "\n",
    "        B, Sq, d_model_Q = Q.shape\n",
    "        # (B, Sm, D) <- (B, Sm, Dm)\n",
    "        Q = self.fc_q(Q)\n",
    "        K = self.fc_k(K)\n",
    "        V = self.fc_v(V)\n",
    "\n",
    "        Q = rearrange(Q, '개 단 헤 차 -> 개 헤 단 차', 헤 = self.H)\n",
    "        K = rearrange(K, '개 단 헤 차 -> 개 헤 단 차', 헤 = self.H)\n",
    "        V = rearrange(V, '개 단 헤 차 -> 개 헤 단 차', 헤 = self.H)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        attention_score = Q @ K.transpose(-2,-1)/self.scale\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        attention = attention_weights @ V\n",
    "\n",
    "        Q = rearrange(attention, '개 헤 단 차 -> 개 단 (헤 차)')\n",
    "        Q = self.fc_o(Q)\n",
    "\n",
    "        return Q, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiModalDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model_A, d_model_V, d_model_C, d_model, dout_p, H, d_ff_C):\n",
    "        super(BiModalDecoderLayer, self).__init__()\n",
    "        # self attention\n",
    "        self.res_layer_self_att = ResidualConnection(d_model_C, dout_p)\n",
    "        self.self_att = MHA(d_model_C, d_model_C, d_model_C, H, dout_p, d_model)\n",
    "        # encoder attention\n",
    "        self.res_layer_enc_att_A = ResidualConnection(d_model_C, dout_p)\n",
    "        self.res_layer_enc_att_V = ResidualConnection(d_model_C, dout_p)\n",
    "        self.enc_att_A = MHA(d_model_C, d_model_A, d_model_A, H, dout_p, d_model)\n",
    "        self.enc_att_V = MHA(d_model_C, d_model_V, d_model_V, H, dout_p, d_model)\n",
    "        # bridge\n",
    "        self.bridge = BridgeConnection(2*d_model_C, d_model_C, dout_p)\n",
    "        # feed forward residual\n",
    "        self.res_layer_ff = ResidualConnection(d_model_C, dout_p)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model_C, d_ff_C, dout_p)\n",
    "\n",
    "    def forward(self, x, masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x (C, memory): C: (B, Sc, Dc) \n",
    "                           memory: (Av: (B, Sa, Da), Va: (B, Sv, Dv))\n",
    "            masks (V_mask: (B, 1, Sv); A_mask: (B, 1, Sa); C_mask (B, Sc, Sc))\n",
    "        Outputs:\n",
    "            x (C, memory): C: (B, Sc, Dc) \n",
    "                           memory: (Av: (B, Sa, Da), Va: (B, Sv, Dv))\n",
    "        '''\n",
    "        C, memory = x\n",
    "        Av, Va = memory\n",
    "\n",
    "        # Define sublayers\n",
    "        # a comment regarding the motivation of the lambda function please see the EncoderLayer\n",
    "        def sublayer_self_att(C): return self.self_att(C, C, C, masks['C_mask'])\n",
    "        def sublayer_enc_att_A(C): return self.enc_att_A(C, Av, Av, masks['A_mask'])\n",
    "        def sublayer_enc_att_V(C): return self.enc_att_V(C, Va, Va, masks['V_mask'])\n",
    "        sublayer_feed_forward = self.feed_forward\n",
    "\n",
    "        # 1. Self Attention\n",
    "        # (B, Sc, Dc)\n",
    "        C = self.res_layer_self_att(C, sublayer_self_att)\n",
    "\n",
    "        # 2. Encoder-Decoder Attention\n",
    "        # (B, Sc, Dc) each\n",
    "        Ca = self.res_layer_enc_att_A(C, sublayer_enc_att_A)\n",
    "        Cv = self.res_layer_enc_att_V(C, sublayer_enc_att_V)\n",
    "        # (B, Sc, 2*Dc)\n",
    "        C = torch.cat([Ca, Cv], dim=-1)\n",
    "        # bridge: (B, Sc, Dc) <- (B, Sc, 2*Dc)\n",
    "        C = self.bridge(C)\n",
    "\n",
    "        # 3. Feed-Forward\n",
    "        # (B, Sc, Dc) <- (B, Sc, Dc)\n",
    "        C = self.res_layer_ff(C, sublayer_feed_forward)\n",
    "\n",
    "        return C, memory\n",
    "\n",
    "\n",
    "\n",
    "class BiModelDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model_A, d_model_V, d_model_C, d_model, dout_p, H, d_ff_C, N):\n",
    "        super(BiModelDecoder, self).__init__()\n",
    "        layer = BiModalDecoderLayer(\n",
    "            d_model_A, d_model_V, d_model_C, d_model, dout_p, H, d_ff_C\n",
    "        )\n",
    "        self.decoder = LayerStack(layer, N)\n",
    "\n",
    "    def forward(self, x, masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x (C, memory): C: (B, Sc, Dc)\n",
    "                           memory: (Av: (B, Sa, Da), Va: (B, Sv, Dv))\n",
    "            masks (V_mask: (B, 1, Sv); A_mask: (B, 1, Sa); C_mask (B, Sc, Sc))\n",
    "        Outputs:\n",
    "            x (C, memory): C: (B, Sc, Dc)\n",
    "                memory: (Av: (B, Sa, Da), Va: (B, Sv, Dv))\n",
    "        '''\n",
    "        # x is (C, memory)\n",
    "        C, memory = self.decoder(x, masks)\n",
    "\n",
    "        return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiModalEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model_M1, d_model_M2, d_model, dout_p, H, d_ff_M1, d_ff_M2):\n",
    "        super(BiModalEncoderLayer, self).__init__()\n",
    "        self.self_att_M1 = MHA(d_model_M1, d_model_M1, d_model_M1, H, dout_p, d_model)\n",
    "        self.self_att_M2 = MHA(d_model_M2, d_model_M2, d_model_M2, H, dout_p, d_model)\n",
    "        self.bi_modal_att_M1 = MHA(d_model_M1, d_model_M2, d_model_M2, H, dout_p, d_model)\n",
    "        self.bi_modal_att_M2 = MHA(d_model_M2, d_model_M1, d_model_M1, H, dout_p, d_model)\n",
    "        self.feed_forward_M1 = PositionwiseFeedForward(d_model_M1, d_ff_M1, dout_p)\n",
    "        self.feed_forward_M2 = PositionwiseFeedForward(d_model_M2, d_ff_M2, dout_p)\n",
    "        self.res_layers_M1 = clone(ResidualConnection(d_model_M1, dout_p), 3)\n",
    "        self.res_layers_M2 = clone(ResidualConnection(d_model_M2, dout_p), 3)\n",
    "\n",
    "    def forward(self, x, masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x (M1, M2): (B, Sm, Dm)\n",
    "            masks (M1, M2): (B, 1, Sm)\n",
    "        Output:\n",
    "            M1m2 (B, Sm1, Dm1), M2m1 (B, Sm2, Dm2),\n",
    "        '''\n",
    "        M1, M2 = x\n",
    "        M1_mask, M2_mask = masks\n",
    "\n",
    "        def sublayer_self_att_M1(M1): return self.self_att_M1(M1, M1, M1, M1_mask)\n",
    "        def sublayer_self_att_M2(M2): return self.self_att_M2(M2, M2, M2, M2_mask)\n",
    "        def sublayer_att_M1(M1): return self.bi_modal_att_M1(M1, M2, M2, M2_mask)\n",
    "        def sublayer_att_M2(M2): return self.bi_modal_att_M2(M2, M1, M1, M1_mask)\n",
    "        sublayer_ff_M1 = self.feed_forward_M1\n",
    "        sublayer_ff_M2 = self.feed_forward_M2\n",
    "\n",
    "        # 1. Self-Attention\n",
    "        # both (B, Sm*, Dm*)\n",
    "        M1 = self.res_layers_M1[0](M1, sublayer_self_att_M1)\n",
    "        M2 = self.res_layers_M2[0](M2, sublayer_self_att_M2)\n",
    "\n",
    "        # 2. Multimodal Attention (var names: M* is the target modality; m* is the source modality)\n",
    "        # (B, Sm1, Dm1)\n",
    "        M1m2 = self.res_layers_M1[1](M1, sublayer_att_M1)\n",
    "        # (B, Sm2, Dm2)\n",
    "        M2m1 = self.res_layers_M2[1](M2, sublayer_att_M2)\n",
    "\n",
    "        # 3. Feed-forward (var names: M* is the target modality; m* is the source modality)\n",
    "        # (B, Sm1, Dm1)\n",
    "        M1m2 = self.res_layers_M1[2](M1m2, sublayer_ff_M1)\n",
    "        # (B, Sm2, Dm2)\n",
    "        M2m1 = self.res_layers_M2[2](M2m1, sublayer_ff_M2)\n",
    "\n",
    "        return M1m2, M2m1\n",
    "\n",
    "\n",
    "class BiModalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model_A, d_model_V, d_model, dout_p, H, d_ff_A, d_ff_V, N):\n",
    "        super(BiModalEncoder, self).__init__()\n",
    "        layer_AV = BiModalEncoderLayer(d_model_A, d_model_V, d_model, dout_p, H, d_ff_A, d_ff_V)\n",
    "        self.encoder_AV = LayerStack(layer_AV, N)\n",
    "\n",
    "    def forward(self, x, masks: dict):\n",
    "        A, V = x\n",
    "\n",
    "        Av, Va = self.encoder_AV((A, V), (masks['A_mask'], masks['V_mask']))\n",
    "\n",
    "        return (Av, Va)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, voc_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, voc_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "class BiModalTransformer(nn.Module):\n",
    "    def __init__(self, d_aud, d_vid, d_model_audio, d_model_video, d_model_caps, train_dataset):\n",
    "        super(BiModalTransformer, self).__init__()\n",
    "\n",
    "\n",
    "        self.emb_A = FeatureEmbedder(d_aud, d_model_audio)\n",
    "        self.emb_V = FeatureEmbedder(d_vid, d_model_video)\n",
    "\n",
    "        self.emb_C = VocabularyEmbedder(train_dataset.trg_voc_size, cfg.d_model_caps)\n",
    "        \n",
    "        self.pos_enc_A = PositionalEncoder(cfg.d_model_audio, cfg.dout_p)\n",
    "        self.pos_enc_V = PositionalEncoder(cfg.d_model_video, cfg.dout_p)\n",
    "        self.pos_enc_C = PositionalEncoder(cfg.d_model_caps, cfg.dout_p)\n",
    "\n",
    "        self.encoder = BiModalEncoder(\n",
    "            d_model_audio, d_model_video, d_model, dout_p, H, \n",
    "            d_ff_audio, d_ff_video, N\n",
    "        )\n",
    "        \n",
    "        self.decoder = BiModelDecoder(\n",
    "            d_model_audio, d_model_video, d_model_caps, d_model, dout_p, \n",
    "            H, d_ff_caps, N\n",
    "        )\n",
    "\n",
    "        self.generator = Generator(d_model_caps, train_dataset.trg_voc_size)\n",
    "\n",
    "        print('initialization: xavier')\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        # initialize embedding after, so it will replace the weights\n",
    "        # of the prev. initialization\n",
    "        self.emb_C.init_word_embeddings(train_dataset.train_vocab.vectors, unfreeze_word_emb)\n",
    "\n",
    "    def forward(self, src: dict, trg, masks: dict):\n",
    "        V, A = src['rgb'] + src['flow'], src['audio']\n",
    "        C = trg\n",
    "\n",
    "        # (B, Sm, Dm) <- (B, Sm, Dm), m in [a, v]; \n",
    "        A = self.emb_A(A)\n",
    "        V = self.emb_V(V)\n",
    "        # (B, Sc, Dc) <- (S, Sc)\n",
    "        C = self.emb_C(C)\n",
    "        \n",
    "        A = self.pos_enc_A(A)\n",
    "        V = self.pos_enc_V(V)\n",
    "        C = self.pos_enc_C(C)\n",
    "        \n",
    "        # notation: M1m2m2 (B, Sm1, Dm1), M1 is the target modality, m2 is the source modality\n",
    "        Av, Va = self.encoder((A, V), masks)\n",
    "\n",
    "        # (B, Sc, Dc)\n",
    "        C = self.decoder((C, (Av, Va)), masks)\n",
    "        \n",
    "        # (B, Sc, Vc) <- (B, Sc, Dc) \n",
    "        C = self.generator(C)\n",
    "\n",
    "        return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
