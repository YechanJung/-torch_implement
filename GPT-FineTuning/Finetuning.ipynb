{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loar config, qnantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quantization_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4'\n",
    ")\n",
    "\n",
    "from peft import LoraConfig\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'result/GPT_small_en.pt'\n",
    "\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, peft_config, is_trainable=True)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "from transformers import MarianTokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ko-en')\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "\n",
    "eos_idx = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Alpaca format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_alpaca_format(instruction, response):\n",
    "    alpaca_format_str = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\\n",
    "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\\\n",
    "    \"\"\"\n",
    "\n",
    "    return alpaca_format_str\n",
    "def prompt_formatting_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        alpaca_formatted_str = convert_to_alpaca_format(instruction, output) + eos_idx\n",
    "        texts.append(alpaca_formatted_str)\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Load\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "no_input_dataset = dataset.filter(lambda example: example['input'] == '')\n",
    "mapped_dataset = no_input_dataset.map(prompt_formatting_func, batched=True)\n",
    "split_dataset = mapped_dataset.train_test_split(test_size=0.01, seed=42)\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "data_collator_param = {}\n",
    "response_template = \"### Response:\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer, mlm=False)\n",
    "data_collator_param[\"data_collator\"] = collator\n",
    "\n",
    "output_dir = \"/fine_tune_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard 설정\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{output_dir}/runs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "  output_dir=output_dir,\n",
    "  report_to = \"tensorboard\",\n",
    "  per_device_train_batch_size = 2,\n",
    "  per_device_eval_batch_size = 2,\n",
    "  gradient_accumulation_steps = 8,\n",
    "  warmup_steps = 50,\n",
    "  max_steps = 100,\n",
    "  eval_steps=10,\n",
    "  save_steps=50,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  save_strategy=\"steps\",\n",
    "  learning_rate = 1e-4,\n",
    "  logging_steps = 1,\n",
    "  optim = \"adamw_8bit\",\n",
    "  weight_decay = 0.01,\n",
    "  lr_scheduler_type = \"constant_with_warmup\",\n",
    "  seed = 42,\n",
    "  gradient_checkpointing = True,\n",
    "  gradient_checkpointing_kwargs={'use_reentrant':True}\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = training_arguments,\n",
    "    **data_collator_param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
